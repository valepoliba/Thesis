{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import random\n",
    "import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "import gc, copy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os, sys\n",
    "p = os.path.abspath('..')\n",
    "sys.path.insert(1, p)\n",
    "\n",
    "from knowledge_graph import *\n",
    "# from rdf2vec import RDF2VecTransformer\n",
    "\n",
    "from lcs_rdf_graph_2_sp_file import LCS\n",
    "\n",
    "from rdf_graph_utils import rdf_to_plot, rdf_to_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def graph_len(g, depth=None):\n",
    "    g = kg_to_rdflib(g, depth) if isinstance(g, KnowledgeGraph) else g\n",
    "    return len(g)\n",
    "\n",
    "\n",
    "def rdflib_to_kg_no_autoref(rdflib_g, label_predicates=[]):\n",
    "    # Iterate over triples, add s, p and o to graph and 2 edges (s-->p, p-->o)\n",
    "    # all predicates in label_predicates get excluded\n",
    "    kg = KnowledgeGraph()\n",
    "    for (s, p, o) in rdflib_g:\n",
    "        if p not in label_predicates:\n",
    "            if s != o:\n",
    "                s_v, o_v = Vertex(str(s)), Vertex(str(o))\n",
    "                p_v = Vertex(str(p), predicate=True)\n",
    "                kg.add_vertex(s_v)\n",
    "                kg.add_vertex(p_v)\n",
    "                kg.add_vertex(o_v)\n",
    "                kg.add_edge(s_v, p_v)\n",
    "                kg.add_edge(p_v, o_v)\n",
    "    return kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... OK\n"
     ]
    }
   ],
   "source": [
    "print(end='Loading data... ', flush=True)\n",
    "g = rdflib.Graph()\n",
    "\n",
    "g.parse('../../datasets/InfluenceTracker/cluster_38-1_hm_ih_lvl2_part.nt', format=\"nt\")\n",
    "# g.parse('../../datasets/InfluenceTracker/full_dataset.nt', format=\"nt\")\n",
    "print('OK')\n",
    "#\n",
    "# # Extract all database accounts' URI\n",
    "all_accounts_file = pd.read_csv('../../datasets/InfluenceTracker/IT_params/users_clusters/accounts_cluster_38-1.txt', sep='\\t')\n",
    "all_accounts = [rdflib.URIRef(x) for x in all_accounts_file['accounts']]\n",
    "#\n",
    "# # Define irrelevant predicates\n",
    "predicates_file = pd.read_csv('../../datasets/InfluenceTracker/IT_params/IT_bad_predicates.tsv', sep='\\t')\n",
    "predicates = [rdflib.URIRef(x) for x in predicates_file['predicate']]\n",
    "#\n",
    "stop_patterns = pd.read_csv('../../datasets/InfluenceTracker/IT_params/IT_stop_patterns.tsv', sep='\\t')\n",
    "stop_patterns = [x for x in stop_patterns['stopping_patterns']]\n",
    "\n",
    "stop_patterns_2 = pd.read_csv('../../datasets/InfluenceTracker/IT_params/IT_stop_patterns_2.tsv', sep='\\t')\n",
    "stop_patterns_2 = [x for x in stop_patterns_2['stopping_patterns']]\n",
    "\n",
    "preds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_dbpedia = False\n",
    "\n",
    "if search_dbpedia:\n",
    "    g_dbpedia = rdflib.Graph()\n",
    "    g_dbpedia.parse(\"../../datasets/InfluenceTracker/dbpedia/cluster_38-1_dbpedia.nt\", format=\"nt\")\n",
    "    \n",
    "    mappings = {}\n",
    "    with open(\"../../datasets/InfluenceTracker/dbpedia/dbpediaGraph.nt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if not line.isspace():\n",
    "                line = line.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                s, p, o, _ = line.split(\" \")\n",
    "                if p == \"http://www.influencetracker.com/ontology#dbpediaUri\":\n",
    "                    mappings[o] = s\n",
    "            \n",
    "\n",
    "    \n",
    "    for (s_db, p_db, o_db) in g_dbpedia:\n",
    "        if str(s_db) in mappings.keys():\n",
    "            new_subj = rdflib.term.URIRef(mappings[str(s_db)].replace(\"TwitterAccount\", \"User\"))\n",
    "            g.add((new_subj, p_db, o_db))\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created kg\n",
      "ok:9\n",
      "not imported: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg = rdflib_to_kg_no_autoref(g, label_predicates=predicates)\n",
    "\n",
    "\n",
    "print(\"created kg\")\n",
    "\n",
    "del g\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "#\n",
    "# # %%\n",
    "# # estraggo un'istanza di knowledge graph per ogni account presente in quello iniziale\n",
    "\n",
    "kg_depth = 2\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "kv = {}\n",
    "accounts = []\n",
    "graphs = []\n",
    "\n",
    "for account in all_accounts:\n",
    "    try:\n",
    "        gi = extract_instance(kg, account, kg_depth)\n",
    "        graphs.append(gi)\n",
    "        accounts.append(account)\n",
    "        # kv.append({'graph': gi, 'resource': account})\n",
    "        kv[str(account)] = i\n",
    "   \n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        j += 1\n",
    "\n",
    "print('ok:' + str(i))\n",
    "print('not imported: ' + str(j))\n",
    "\n",
    "del kg\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cluster = []\n",
    "clusters = {}\n",
    "index_to_name = {}\n",
    "resources = []\n",
    "path_cluster = \"../../datasets/InfluenceTracker/data/cluster_line_38-1\"\n",
    "\n",
    "with open(path_cluster, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"TwitterAccount\", \"User\").strip()\n",
    "        if line in kv.keys():\n",
    "            cluster.append(kv[line])\n",
    "            index_to_name[kv[line]] = line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.influencetracker.com/resource/User/airbnb con dim 2173\n",
      "1 http://www.influencetracker.com/resource/User/britneyspears con dim 304\n",
      "2 http://www.influencetracker.com/resource/User/twitter con dim 3274\n",
      "3 http://www.influencetracker.com/resource/User/fortunemagazine con dim 220\n",
      "4 http://www.influencetracker.com/resource/User/yahoo con dim 409\n",
      "5 http://www.influencetracker.com/resource/User/theellenshow con dim 465\n",
      "6 http://www.influencetracker.com/resource/User/whitehouse con dim 927\n",
      "7 http://www.influencetracker.com/resource/User/pinterest con dim 208\n",
      "8 http://www.influencetracker.com/resource/User/billgates con dim 120\n",
      "esploro il cluster: 45 con dimensione 9\n"
     ]
    }
   ],
   "source": [
    "# print(list(kv.keys())[list(kv.values()).index(0)])\n",
    "\n",
    "for i in range(len(cluster)):\n",
    "    print(str(i) + \" \" + list(kv.keys())[list(kv.values()).index(i)] + \" con dim \" + str(len(kg_to_rdflib(graphs[i], kg_depth))))\n",
    "\n",
    "directory = \"../../outputs/IT/oAccounts_cluster_38-1\"\n",
    "\n",
    "#if not os.path.exists(directory):\n",
    "#    os.mkdir(directory)\n",
    "\n",
    "Path(\"../../outputs/IT/oAccounts_cluster_38-1\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# cluster da analizzare\n",
    "k = 45\n",
    "\n",
    "clusters[k] = cluster\n",
    "\n",
    "clusters[k] = sorted(clusters[k],  key=lambda res: len(kg_to_rdflib(graphs[res], kg_depth)))\n",
    "\n",
    "# clusters[k] = [48, 50]\n",
    "\n",
    "L = len(clusters[k])\n",
    "\n",
    "print(\"esploro il cluster: \" + str(k) + \" con dimensione \" + str(L))\n",
    "\n",
    "# risorsa iniziale\n",
    "# resource_1 = random.choice(clusters[k])\n",
    "\n",
    "resource_1 = clusters[k].pop(0)\n",
    "graph_1 = graphs[resource_1]\n",
    "\n",
    "\n",
    "# clusters[k].remove(resource_1)\n",
    "explored_resources = [resource_1]\n",
    "\n",
    "\n",
    "iteration = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS with resource: 7 con dim: 208\n",
      "starting graphs exploration...\n",
      "exploration ended.\n",
      "filling LCS graph.\n",
      "graph completed.\n",
      "dim LCS itermedio tra [8, 7] \n",
      "--> 120\n",
      "Iterazione: 0, trovato LCS tra: [8, 7]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m graph_1 \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(seed)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIterazione: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(iteration) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, trovato LCS tra: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(explored_resources))\n\u001b[1;32m---> 30\u001b[0m rdf_to_text(seed, directory, \u001b[39m\"\u001b[39;49m\u001b[39mturtle\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtmp_LCS_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(iteration))\n\u001b[0;32m     31\u001b[0m rdf_to_text(seed, directory, \u001b[39m\"\u001b[39m\u001b[39mnt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtmp_LCS_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(iteration))\n\u001b[0;32m     33\u001b[0m \u001b[39mdel\u001b[39;00m seed\n",
      "File \u001b[1;32md:\\Università\\Magistrale\\Magistrale\\Tesi\\Thesis\\src\\rdf_graph_utils.py:40\u001b[0m, in \u001b[0;36mrdf_to_text\u001b[1;34m(graph, path, format, name)\u001b[0m\n\u001b[0;32m     38\u001b[0m strings \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mserialize(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 40\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(strings)\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# file = open(directory + \"/tmp_LCS.log\", \"a\")\n",
    "\n",
    "while clusters[k] and graph_len(graph_1, kg_depth) > 0:\n",
    "    # try:\n",
    "        file = open(directory + \"/tmp_LCS.log\", 'a')\n",
    "        \n",
    "        # resource_2 = random.choice(clusters[k])\n",
    "   \n",
    "        resource_2 = clusters[k].pop(0)\n",
    "        graph_2 = graphs[resource_2]\n",
    "        print(\"LCS with resource: \" + str(resource_2) \n",
    "              + \" con dim: \" + str(graph_len(graph_2, kg_depth)))\n",
    "        # clusters[k].remove(resource_2)\n",
    "\n",
    "        # seed\n",
    "        seed = LCS(graph_1, graph_2, depth=2, stop_patterns=stop_patterns, uninformative_triples=preds, additional_stop_patterns=stop_patterns_2)\n",
    "        seed.find()\n",
    "              \n",
    "        explored_resources.append(resource_2)\n",
    "        \n",
    "        print(\"dim LCS itermedio tra \" + str(explored_resources) + \" \\n--> \" + str(graph_len(seed)))\n",
    "        \n",
    "        del graph_1\n",
    "        del graph_2\n",
    "        gc.collect()\n",
    "        \n",
    "        graph_1 = copy.deepcopy(seed)\n",
    "\n",
    "        print(\"Iterazione: \" + str(iteration) + \", trovato LCS tra: \" + str(explored_resources))\n",
    "        rdf_to_text(seed, directory, \"turtle\", \"tmp_LCS_\" + str(iteration))\n",
    "        rdf_to_text(seed, directory, \"nt\", \"tmp_LCS_\" + str(iteration))\n",
    "        \n",
    "        del seed\n",
    "        gc.collect()\n",
    "        \n",
    "        file.write(\"Iterazione: \" + str(iteration) +  \" risorse esplorate: \" + str(explored_resources) + \"\\n\")\n",
    "        file.close()\n",
    "        \n",
    "        iteration += 1\n",
    "        \"\"\"\n",
    "        except Exception as e:\n",
    "            print(\"got exception: \" + str(e))\n",
    "            file.close()\n",
    "        \"\"\"\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rdf_to_plot(graph_1, directory)\n",
    "print(\"LCS finale tra le risorse: \" + str(explored_resources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SALVATAGGIO DEL GRAFO\n",
    "\n",
    "rdf_to_text(graph_1, directory, 'turtle', 'final_LCS')\n",
    "rdf_to_text(graph_1, directory, 'nt', 'final_LCS')\n",
    "file =  open(directory + \"/final_LCS_resources.log\", \"a\")\n",
    "file.write(str(explored_resources))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file.write(\"\\n ####### \\n\")\n",
    "\n",
    "explored_resources_names = []\n",
    "\n",
    "for account_index in explored_resources:\n",
    "    account_name = index_to_name[account_index]\n",
    "    explored_resources_names.append(account_name)\n",
    "\n",
    "file.write(str(explored_resources_names))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "f337ac061ce9696b0f1ec4c1d557fc964de9fc8c97f461bc871446d45fc96448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
